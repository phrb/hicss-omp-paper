\section{Introduction}
\label{sec:introduction}

There is an ongoing evolution of parallel processing platforms. The pursuit for high performance in general-purpose and scientific applications have turned to the exploration of the parallel processing capabilities provided by multi-core processors and arrangements of coprocessors like the Xeon Phi~\cite{xeonphiRuntime}. The machine currently at the top of the \textit{TOP500} list~\cite{dongarra1997top500} uses multi-core processors (12 cores) and Xeon Phi coprocessors~\cite{top500:site}.
Multi-core processors have become popular on personal computers and mobile devices. This popularization increases the need for specialized developers able to work in parallel applications and apply parallel computing models~\cite{Chen:2009:ChallengesMIC}~\cite{okur2012developers}.

Today there is a wide gap between legacy code applications that could benefit from parallel computing power and the new parallel computing platforms that could provide that power. However, legacy applications are generally sequential and therefore not easily scalable to multi-core architectures and parallel computing platforms. 

Asanovic \textit{et al.} \cite{Asanovic2009} describes this scenario, and defends the need for Structural and Computational Patterns in Parallel Computing. These patterns allow the portability of parallel programming solutions, while also allowing specialization of applications with the same behavior. In some situations it is not a matter of achieving the highest possible performance, but instead of making the application achieve acceptable performance, benefiting from a new parallel computing platform~\cite{Asanovic2009}. A similar approach is proposed by Pllana \textit{et al.} in a development environment that uses a combination of Model-Driven Development and Parallel Building Blocks \cite{Pllana:2009}. The approach consists in creating the model and then generating or transforming the code using \emph{model-to-source} tools.

In this context, the approach of using Compilation Directives to guide the compiler in the parallelization process has gained popularity. These directives are implemented using the compiler's pre-processing directives and are utilized as annotations that provide tips about the sequential code. Among the tools and extensions that use compilation directives are the \texttt{OpenMP} API \cite{Dagum1998a} \cite{Chapman:2007} used for writing programs for multi-core architectures, and the \texttt{OpenACC} programming standard~\cite{openacc:api}, used for writing programs for heterogeneous CPU/GPU architectures. The \texttt{OpenMP 4.0} specification supports offloading capabilities \cite{openmp:api:2013} such as \texttt{OpenACC} and \texttt{AMD HSA}~\cite{amd:hsa:site}. These tools aim to make easier the task of designing and implementing efficient and provably correct parallel algorithms. However, the abstraction layers built by the extensions also have the potential to conceal architectural details and to generate new parallel programming challenges.

When comparing \texttt{OpenMP} to other available tools for developing multi-threaded applications such as the \texttt{pthreads} interface, the API can be considered almost costless. However, when writing more complex applications this may not be true. The design and implementation of such complex applications require more knowledge about platforms and execution models than what can be shown by tutorials, that present high level and trivial examples. These tutorials try to convince the user that programming with directives is simple and easy to do, which is not always true. For instance, to achieve good performance in some cases it is necessary to describe many restrictions on annotations~\cite{OpenMPTasks2009}~\cite{mattson2003good}.

Krawezik and Cappello~\cite{CPE:CPE905} evidenced this difficulty by comparing MPI and three OpenMP versions of benchmarks. The most popular use of \texttt{OpenMP} directives is at the loop level. The programmer discovers potentially parallel loops and annotates the code with directives starting and closing parallel regions. Good results were obtained only when SPMD was implemented on OpenMP, and achieving good performance required a significant programming effort~\cite{CPE:CPE905}.

In this work we have gathered data from assignments made by graduate students for the \emph{Introduction to Parallel and Distributed Computing} course. Among other exercises, the students were asked to find examples of \texttt{OpenMP} code in tutorials and official manuals that failed under special conditions. These conditions could be created by, for example, placing barriers or forcing racing conditions by exposing inaccurate thread management.

We argue that it is necessary to consider fundamental concepts of architectures and parallel programming techniques when teaching students new to the subject, so that they can learn when and how to apply the programming models~\cite{ben1999thinking}. The abstractions provided by parallel programming tools over concepts such as threads, barriers and loop parallelization should be carefully studied, analysed and conceptually built in the classroom before they are used by the students in their assignments.

We found eight tutorials with different kinds of \texttt{OpenMP} errors in the experiment with the students' assignments. Many of these errors where similar to the most common \texttt{OpenMP} mistakes~\cite{SuB:2005:CMO:1892830.1892863}. The most common type of error identified in our experiment was the management of shared or private variables. This problem is common across multiple programming paradigms.

The \textbf{main contribution of this work} is to show the importance of teaching core architecture and parallel programming concepts thoroughly, despite the availability of powerful tools such as \texttt{OpenMP}. We also aim to show that the \texttt{OpenMP} API is not as easy to use as it might seem.

The rest of the paper is organized as follows. The following section discusses related work. Section~\ref{sec:concurrency} discusses concurrency and other concepts in parallel computing. Section~\ref{sec:directives} presents the idea of compilation directives. Section~\ref{sec:methodology} discusses the methodology applied in the experiments, and Section~\ref{sec:results} presents our results. Section~\ref{sec:students:perception} presents the perceptions of the graduate students regarding the exercise and learning \texttt{OpenMP}. Finally, Section~\ref{sec:conclusions} presents the conclusions and next steps.

% Institution 2 = University of SÃ£o Paulo