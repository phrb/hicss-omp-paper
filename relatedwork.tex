\section{Related Work}
\label{sec:relatedwork}

There has been a large body of works that have presented results of studies and experiments in teaching parallel computing courses. S\"{u}\ss~and Leopold~\cite{SuB:2005:CMO:1892830.1892863} discuss the teaching of parallel computing, in particular the use of \texttt{OpenMP}. The authors present the most common errors identified in assignments given in parallel computing classes. The errors were classified in two categories: \emph{correctness mistakes} and \emph{performance mistakes}. Errors that impact the correctness of a program are errors of the first category. Errors which make programs perform slower are classified in the second category. The work also provides a checklist containing tips to avoid the most common mistakes.

Falcao~\cite{6565518} defends that teaching a parallel programming model or interface such as \texttt{OpenMP} in the beginning of undergraduate courses is not much harder than teaching a language like C or C++. The benefits of teaching these skills early on in the course would be interesting, considering that the majority of computer architectures are now are multi-core. Parallel computing should be taught with the use of higher level tools. One of the approaches proposed by Ferner \textit{et al.}~\cite{6651019} is to use compiler directives to describe how a program should be parallelized.

The need for Structural and Computational Patterns to Parallel Computing is defended by Asanovic \textit{et al.}~\cite{Asanovic2009} as the way to allow portability of solutions and the specialization of applications with the same behavior. Using patterns to design applications make it easy to change the target platform. The applications will be bound to platforms in low levels of architecture and probably inside a specialized function which can be easily changed. 

A similar approach is proposed by Pllana \textit{et al.}~\cite{Pllana:2009} in a programming environment that uses a combination of Model-Driven Development and Parallel Building Blocks. The idea is to create the model and then generate or transform the code using model-to-source tools. The proposed environment uses intelligent agents to assist the programmer, this is done during the high level program composition and when the programmer loads a BLAS function for some dense linear algebra operation, the tool suggests the use of the appropriate parallel basic block. The parallelism is inside the parallel building blocks and the programmer is not exposed directly to the complexity of parallel programming problems.

Speyer \textit{et al.}~\cite{4755913} defend that implementing applications with high performance is harder than ever, specially due to the emergence of multi-core architectures. This leads to the development of hybrid applications that use both the \texttt{OpenMP} and \texttt{MPI} (\emph{Message Passing Interface}) APIs. This presents even greater challenges to the programmers, since they now need to know how to use both code annotations and message passing abstractions to achieve high performance. The paper then compares different parallel programming tools with regard to their ease of use. There is a substantial effort to start writing parallel and high performance code even with these tools.

Some problems can be solved using different directive combinations, and the chosen combination influences the code that is generated. The \texttt{Parallware}~\cite{Parallware:2014} source-to-source compiler showed how different strategies and uses of directive combinations can lead to different performance results. The use of the wrong \texttt{OpenMP} directives and strategies can significantly reduce the performance over a sequential version.

The \texttt{OmpVerify}~\cite{Basupalli:2011} system implements a static analysis tool based on the polyhedral model to extract the parallel portions of a program automatically and to detect important classes of common data-race errors in \texttt{OpenMP} \texttt{parallel for} directives. Saillard \textit{et al.}~\cite{Saillard:2014} statically validates barriers and work sharing constructs to avoid the improper use of directives that could cause deadlocks. S\"{u}\ss~and Leopold~\cite{Leopold:userOpenMP}, present an user's experience with parallel sorting and OpenMP. They have used several versions of a simple parallel sorting algorithm to show some weaknesses of OpenMP.

Substantial efforts have been made to achieve the simplest possible way to parallelize code, and the task is not easy. In this paper we present an exploratory analysis of common \texttt{OpenMP} errors, confirming that programming with high-level APIs and abstractions can still be difficult.